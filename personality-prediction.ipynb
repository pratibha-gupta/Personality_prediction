{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom numpy import asarray\nfrom numpy import savetxt\nfrom numpy import loadtxt\nimport pickle as pkl\nfrom scipy import sparse\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Text Processing\nimport re\nimport itertools\nimport string\nimport collections\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\n# Machine Learning packages\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport sklearn.cluster as cluster\nfrom sklearn.manifold import TSNE\n\n# Model training and evaluation\nfrom sklearn.model_selection import train_test_split\n\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n#Metrics\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# Ignore noise warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:06.393512Z","iopub.execute_input":"2022-03-27T20:46:06.393846Z","iopub.status.idle":"2022-03-27T20:46:06.407772Z","shell.execute_reply.started":"2022-03-27T20:46:06.393816Z","shell.execute_reply":"2022-03-27T20:46:06.407055Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/mbti-type/mbti_1.csv\")\nd = pd.read_csv(\"../input/mbti-type/mbti_1.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:06.445512Z","iopub.execute_input":"2022-03-27T20:46:06.446557Z","iopub.status.idle":"2022-03-27T20:46:07.989045Z","shell.execute_reply.started":"2022-03-27T20:46:06.446425Z","shell.execute_reply":"2022-03-27T20:46:07.988194Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"print(df[\"posts\"][0])","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:07.991077Z","iopub.execute_input":"2022-03-27T20:46:07.991584Z","iopub.status.idle":"2022-03-27T20:46:07.997640Z","shell.execute_reply.started":"2022-03-27T20:46:07.991538Z","shell.execute_reply":"2022-03-27T20:46:07.996820Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"df[\"type\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:07.999082Z","iopub.execute_input":"2022-03-27T20:46:07.999590Z","iopub.status.idle":"2022-03-27T20:46:08.014411Z","shell.execute_reply.started":"2022-03-27T20:46:07.999548Z","shell.execute_reply":"2022-03-27T20:46:08.013561Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:08.016544Z","iopub.execute_input":"2022-03-27T20:46:08.017059Z","iopub.status.idle":"2022-03-27T20:46:08.033734Z","shell.execute_reply.started":"2022-03-27T20:46:08.017013Z","shell.execute_reply":"2022-03-27T20:46:08.032767Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"print(df[df[\"type\"]== \"ESTJ\"][\"posts\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:08.035149Z","iopub.execute_input":"2022-03-27T20:46:08.035822Z","iopub.status.idle":"2022-03-27T20:46:08.051443Z","shell.execute_reply.started":"2022-03-27T20:46:08.035778Z","shell.execute_reply":"2022-03-27T20:46:08.050398Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df[\"length_posts\"] = df[\"posts\"].apply(len)\nsns.distplot(df[\"length_posts\"]).set_title(\"Distribution of Lengths of all 50 Posts\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:08.052716Z","iopub.execute_input":"2022-03-27T20:46:08.053511Z","iopub.status.idle":"2022-03-27T20:46:08.453553Z","shell.execute_reply.started":"2022-03-27T20:46:08.053465Z","shell.execute_reply":"2022-03-27T20:46:08.452753Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"words = list(df[\"posts\"].apply(lambda x: x.split()))\nwords = [x for y in words for x in y]\nCounter(words).most_common(40)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:08.454609Z","iopub.execute_input":"2022-03-27T20:46:08.455338Z","iopub.status.idle":"2022-03-27T20:46:12.821454Z","shell.execute_reply.started":"2022-03-27T20:46:08.455306Z","shell.execute_reply":"2022-03-27T20:46:12.820488Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def extract(posts, new_posts):\n    for post in posts[1].split(\"|||\"):\n        new_posts.append((posts[0], post))\n\nposts = []\ndf.apply(lambda x: extract(x, posts), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:12.822620Z","iopub.execute_input":"2022-03-27T20:46:12.822856Z","iopub.status.idle":"2022-03-27T20:46:14.798389Z","shell.execute_reply.started":"2022-03-27T20:46:12.822828Z","shell.execute_reply":"2022-03-27T20:46:14.797433Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"posts[0:5]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:14.799446Z","iopub.execute_input":"2022-03-27T20:46:14.800081Z","iopub.status.idle":"2022-03-27T20:46:14.806310Z","shell.execute_reply.started":"2022-03-27T20:46:14.800047Z","shell.execute_reply":"2022-03-27T20:46:14.805339Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(df, remove_special=True):\n    texts = df['posts'].copy()\n    labels = df['type'].copy()\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n    #Strip Punctation\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[\\.+]', \".\",x))\n    #Remove multiple fullstops\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n    #Remove Non-words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n    #Convert posts to lowercase\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n    #Remove multiple letter repeating words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x)) \n    #Remove very short or long words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x)) \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n    #Remove MBTI Personality Words - crutial in order to get valid model accuracy estimation for unseen data. \n    if remove_special:\n        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n        pers_types = [p.lower() for p in pers_types]\n        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n        print(p)\n    return df\n#Preprocessing of entered Text\nnew_df = preprocess_text(d)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:46:14.809696Z","iopub.execute_input":"2022-03-27T20:46:14.810524Z","iopub.status.idle":"2022-03-27T20:47:11.248772Z","shell.execute_reply.started":"2022-03-27T20:46:14.810484Z","shell.execute_reply":"2022-03-27T20:47:11.247291Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"new_df","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:11.252449Z","iopub.execute_input":"2022-03-27T20:47:11.252824Z","iopub.status.idle":"2022-03-27T20:47:11.269662Z","shell.execute_reply.started":"2022-03-27T20:47:11.252768Z","shell.execute_reply":"2022-03-27T20:47:11.268771Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"min_words = 15\nprint(\"Before : Number of posts\", len(new_df)) \nnew_df[\"no. of. words\"] = new_df[\"posts\"].apply(lambda x: len(re.findall(r'\\w+', x)))\nnew_df = new_df[new_df[\"no. of. words\"] >= min_words]\n\nprint(\"After : Number of posts\", len(new_df))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:11.271171Z","iopub.execute_input":"2022-03-27T20:47:11.271840Z","iopub.status.idle":"2022-03-27T20:47:13.432303Z","shell.execute_reply.started":"2022-03-27T20:47:11.271790Z","shell.execute_reply":"2022-03-27T20:47:13.431259Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"enc = LabelEncoder()\nnew_df['type of encoding'] = enc.fit_transform(new_df['type'])\n\ntarget = new_df['type of encoding'] ","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:13.433733Z","iopub.execute_input":"2022-03-27T20:47:13.433995Z","iopub.status.idle":"2022-03-27T20:47:13.444571Z","shell.execute_reply.started":"2022-03-27T20:47:13.433966Z","shell.execute_reply":"2022-03-27T20:47:13.443531Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"vect = CountVectorizer(stop_words='english') \n\n# Converting posts (or training or X feature) into numerical form by count vectorization\ntrain =  vect.fit_transform(new_df[\"posts\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:13.446037Z","iopub.execute_input":"2022-03-27T20:47:13.446253Z","iopub.status.idle":"2022-03-27T20:47:20.255728Z","shell.execute_reply.started":"2022-03-27T20:47:13.446229Z","shell.execute_reply":"2022-03-27T20:47:20.254672Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.3, stratify=target, random_state=42)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:20.257094Z","iopub.execute_input":"2022-03-27T20:47:20.257388Z","iopub.status.idle":"2022-03-27T20:47:20.283482Z","shell.execute_reply.started":"2022-03-27T20:47:20.257360Z","shell.execute_reply":"2022-03-27T20:47:20.282552Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/mbti-type/mbti_1.csv\")\ndata","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:20.284813Z","iopub.execute_input":"2022-03-27T20:47:20.285098Z","iopub.status.idle":"2022-03-27T20:47:21.129515Z","shell.execute_reply.started":"2022-03-27T20:47:20.285069Z","shell.execute_reply":"2022-03-27T20:47:21.128664Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def get_types(row):\n    t=row['type']\n\n    I = 0; N = 0\n    T = 0; J = 0\n    I = 1 if (t[0] == 'I') else 0\n    N = 1 if (t[1] == 'N') else 0\n    T = 1 if (t[2] == 'T') else 0\n    J = 1 if (t[3] == 'J') else 0\n    \n    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n\ndata = data.join(data.apply (lambda row: get_types (row),axis=1))\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:21.131015Z","iopub.execute_input":"2022-03-27T20:47:21.131261Z","iopub.status.idle":"2022-03-27T20:47:24.559929Z","shell.execute_reply.started":"2022-03-27T20:47:21.131223Z","shell.execute_reply":"2022-03-27T20:47:24.558998Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:24.561309Z","iopub.execute_input":"2022-03-27T20:47:24.561554Z","iopub.status.idle":"2022-03-27T20:47:24.576759Z","shell.execute_reply.started":"2022-03-27T20:47:24.561525Z","shell.execute_reply":"2022-03-27T20:47:24.575628Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"lemmatiser = WordNetLemmatizer()\n\n# Remove the stop words for speed \nuseless_words = stopwords.words(\"english\")\n\n# Remove these from the posts\nunique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\nunique_type_list = [x.lower() for x in unique_type_list]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:24.578110Z","iopub.execute_input":"2022-03-27T20:47:24.578445Z","iopub.status.idle":"2022-03-27T20:47:24.585981Z","shell.execute_reply.started":"2022-03-27T20:47:24.578414Z","shell.execute_reply":"2022-03-27T20:47:24.585150Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\nb_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n\ndef translate_personality(personality):\n    # transform mbti to binary vector\n    return [b_Pers[l] for l in personality]\n\n#To show result output for personality prediction\ndef translate_back(personality):\n    # transform binary vector to mbti personality\n    s = \"\"\n    for i, l in enumerate(personality):\n        s += b_Pers_list[i][l]\n    return s\n\nlist_personality_bin = np.array([translate_personality(p) for p in data.type])\nprint(\"Binarize MBTI list: \\n%s\" % list_personality_bin)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:24.587164Z","iopub.execute_input":"2022-03-27T20:47:24.587732Z","iopub.status.idle":"2022-03-27T20:47:24.628451Z","shell.execute_reply.started":"2022-03-27T20:47:24.587689Z","shell.execute_reply":"2022-03-27T20:47:24.627473Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True):\n    list_personality = []\n    list_posts = []\n    len_data = len(data)\n    i=0\n  \n    for row in data.iterrows():\n        posts = row[1].posts\n\n        #Remove url links \n        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n\n        #Remove Non-words - keep only words\n        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n\n        # Remove spaces > 1\n        temp = re.sub(' +', ' ', temp).lower()\n\n        #Remove multiple letter repeating words\n        temp = re.sub(r'([a-z])\\1{2,}[\\s|\\w]*', '', temp)\n\n        #Remove stop words\n        if remove_stop_words:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in useless_words])\n        else:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n          \n        #Remove MBTI personality words from posts\n        if remove_mbti_profiles:\n            for t in unique_type_list:\n                temp = temp.replace(t,\"\")\n\n        # transform mbti to binary vector\n        type_labelized = translate_personality(row[1].type) #or use lab_encoder.transform([row[1].type])[0]\n        list_personality.append(type_labelized)\n        # the cleaned data temp is passed here\n        list_posts.append(temp)\n\n  # returns the result\n    list_posts = np.array(list_posts)\n    list_personality = np.array(list_personality)\n    return list_posts, list_personality\n\nlist_posts, list_personality  = pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:47:24.629681Z","iopub.execute_input":"2022-03-27T20:47:24.630089Z","iopub.status.idle":"2022-03-27T20:48:21.711567Z","shell.execute_reply.started":"2022-03-27T20:47:24.630044Z","shell.execute_reply":"2022-03-27T20:48:21.710532Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"len(list_posts)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:48:21.712822Z","iopub.execute_input":"2022-03-27T20:48:21.713094Z","iopub.status.idle":"2022-03-27T20:48:21.719248Z","shell.execute_reply.started":"2022-03-27T20:48:21.713063Z","shell.execute_reply":"2022-03-27T20:48:21.718309Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"len(list_personality)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:48:21.720525Z","iopub.execute_input":"2022-03-27T20:48:21.720831Z","iopub.status.idle":"2022-03-27T20:48:21.736604Z","shell.execute_reply.started":"2022-03-27T20:48:21.720801Z","shell.execute_reply":"2022-03-27T20:48:21.735653Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vectorizing the database posts to a matrix of token counts for the model\ncntizer = CountVectorizer(analyzer=\"word\", \n                             max_features=1000,  \n                             max_df=0.7,\n                             min_df=0.1) \n# the feature should be made of word n-gram \n# Learn the vocabulary dictionary and return term-document matrix\nprint(\"Using CountVectorizer :\")\nX_cnt = cntizer.fit_transform(list_posts)\n\n#The enumerate object yields pairs containing a count and a value (useful for obtaining an indexed list)\nfeature_names = list(enumerate(cntizer.get_feature_names()))\nprint(\"10 feature names can be seen below\")\nprint(feature_names[0:10])\n\n# For the Standardization or Feature Scaling Stage :-\n# Transform the count matrix to a normalized tf or tf-idf representation\ntfizer = TfidfTransformer()\n\n# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\nprint(\"\\nUsing Tf-idf :\")\n\nprint(\"Now the dataset size is as below\")\nX_tfidf =  tfizer.fit_transform(X_cnt).toarray()\nprint(X_tfidf.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:48:21.738512Z","iopub.execute_input":"2022-03-27T20:48:21.739079Z","iopub.status.idle":"2022-03-27T20:48:26.776827Z","shell.execute_reply.started":"2022-03-27T20:48:21.739034Z","shell.execute_reply":"2022-03-27T20:48:26.775751Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"personality_type = [ \"IE: Introversion (I) / Extroversion (E)\", \"NS: Intuition (N) / Sensing (S)\", \n                   \"FT: Feeling (F) / Thinking (T)\", \"JP: Judging (J) / Perceiving (P)\"  ]\n\nfor l in range(len(personality_type)):\n    print(personality_type[l])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:48:26.778216Z","iopub.execute_input":"2022-03-27T20:48:26.778524Z","iopub.status.idle":"2022-03-27T20:48:26.785716Z","shell.execute_reply.started":"2022-03-27T20:48:26.778496Z","shell.execute_reply":"2022-03-27T20:48:26.784405Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"for l in range(len(personality_type)):\n    \n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, Y, test_size=0.33, random_state=7)\n\n    # fit model on training data\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    \n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    \n    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:48:26.787317Z","iopub.execute_input":"2022-03-27T20:48:26.788041Z","iopub.status.idle":"2022-03-27T20:48:54.890737Z","shell.execute_reply.started":"2022-03-27T20:48:26.788008Z","shell.execute_reply":"2022-03-27T20:48:54.889439Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_tfidf, Y, test_size=0.33, random_state=7)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:48:54.892206Z","iopub.execute_input":"2022-03-27T20:48:54.892504Z","iopub.status.idle":"2022-03-27T20:48:54.908497Z","shell.execute_reply.started":"2022-03-27T20:48:54.892474Z","shell.execute_reply":"2022-03-27T20:48:54.907481Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:48:54.914634Z","iopub.execute_input":"2022-03-27T20:48:54.915152Z","iopub.status.idle":"2022-03-27T20:48:54.919495Z","shell.execute_reply.started":"2022-03-27T20:48:54.915113Z","shell.execute_reply":"2022-03-27T20:48:54.918702Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"model = keras.Sequential()\n# Add an Embedding layer expecting input vocab of size 1000, and\n# output embedding dimension of size 64.\nmodel.add(layers.Embedding(input_dim=595, output_dim=1))\n\n# Add a LSTM layer with 128 internal units.\nmodel.add(layers.LSTM(128))\n\n# Add a Dense layer with 10 units.\nmodel.add(layers.Dense(10))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:48:54.920823Z","iopub.execute_input":"2022-03-27T20:48:54.921360Z","iopub.status.idle":"2022-03-27T20:48:55.209975Z","shell.execute_reply.started":"2022-03-27T20:48:54.921329Z","shell.execute_reply":"2022-03-27T20:48:55.208962Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:48:55.211734Z","iopub.execute_input":"2022-03-27T20:48:55.212210Z","iopub.status.idle":"2022-03-27T20:48:55.226348Z","shell.execute_reply.started":"2022-03-27T20:48:55.212178Z","shell.execute_reply":"2022-03-27T20:48:55.225222Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:48:55.227522Z","iopub.execute_input":"2022-03-27T20:48:55.227775Z","iopub.status.idle":"2022-03-27T20:50:43.961736Z","shell.execute_reply.started":"2022-03-27T20:48:55.227736Z","shell.execute_reply":"2022-03-27T20:50:43.961022Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:50:43.963403Z","iopub.execute_input":"2022-03-27T20:50:43.963761Z","iopub.status.idle":"2022-03-27T20:51:03.402348Z","shell.execute_reply.started":"2022-03-27T20:50:43.963734Z","shell.execute_reply":"2022-03-27T20:51:03.401522Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"y_pred.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:03.406345Z","iopub.execute_input":"2022-03-27T20:51:03.408019Z","iopub.status.idle":"2022-03-27T20:51:03.415489Z","shell.execute_reply.started":"2022-03-27T20:51:03.407983Z","shell.execute_reply":"2022-03-27T20:51:03.414782Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# accuracy = accuracy_score(y_test, y_pred)\n# accuracy","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:03.417256Z","iopub.execute_input":"2022-03-27T20:51:03.419149Z","iopub.status.idle":"2022-03-27T20:51:03.427434Z","shell.execute_reply.started":"2022-03-27T20:51:03.419102Z","shell.execute_reply":"2022-03-27T20:51:03.426484Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:03.428914Z","iopub.execute_input":"2022-03-27T20:51:03.429277Z","iopub.status.idle":"2022-03-27T20:51:03.445142Z","shell.execute_reply.started":"2022-03-27T20:51:03.429235Z","shell.execute_reply":"2022-03-27T20:51:03.444298Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/mbti-type/mbti_1.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:03.448750Z","iopub.execute_input":"2022-03-27T20:51:03.449397Z","iopub.status.idle":"2022-03-27T20:51:04.221520Z","shell.execute_reply.started":"2022-03-27T20:51:03.449358Z","shell.execute_reply":"2022-03-27T20:51:04.220487Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"types = np.unique(data.type.values)\ndef get_type_index(string):\n    return list(types).index(string)\ndata['type_index'] = data['type'].apply(get_type_index)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:04.222905Z","iopub.execute_input":"2022-03-27T20:51:04.223176Z","iopub.status.idle":"2022-03-27T20:51:04.261657Z","shell.execute_reply.started":"2022-03-27T20:51:04.223144Z","shell.execute_reply":"2022-03-27T20:51:04.261027Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"import string\nimport re\n\ndef clean_text(text):\n    regex = re.compile('[%s]' % re.escape('|'))\n    text = regex.sub(\" \", text)\n    words = str(text).split()\n    words = [i.lower() + \" \" for i in words]\n    words = [i for i in words if not \"http\" in i]\n    words = \" \".join(words)\n    words = words.translate(words.maketrans('', '', string.punctuation))\n    return words","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:04.264999Z","iopub.execute_input":"2022-03-27T20:51:04.265277Z","iopub.status.idle":"2022-03-27T20:51:04.272430Z","shell.execute_reply.started":"2022-03-27T20:51:04.265246Z","shell.execute_reply":"2022-03-27T20:51:04.271554Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"data['cleaned_text'] = data['posts'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:04.273756Z","iopub.execute_input":"2022-03-27T20:51:04.274179Z","iopub.status.idle":"2022-03-27T20:51:09.747615Z","shell.execute_reply.started":"2022-03-27T20:51:04.274138Z","shell.execute_reply":"2022-03-27T20:51:09.746772Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data)\ntrain, val = train_test_split(train)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:09.748864Z","iopub.execute_input":"2022-03-27T20:51:09.749122Z","iopub.status.idle":"2022-03-27T20:51:09.760529Z","shell.execute_reply.started":"2022-03-27T20:51:09.749093Z","shell.execute_reply":"2022-03-27T20:51:09.759560Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size = 10000\ntrunc_type = \"post\"\npad_type = \"post\"\noov_tok = \"<OOV>\"\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(data.cleaned_text.values)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:09.761499Z","iopub.execute_input":"2022-03-27T20:51:09.761731Z","iopub.status.idle":"2022-03-27T20:51:20.787610Z","shell.execute_reply.started":"2022-03-27T20:51:09.761704Z","shell.execute_reply":"2022-03-27T20:51:20.786466Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"maxlen = 1500\ntrain_sequences = tokenizer.texts_to_sequences(train.cleaned_text.values)\ntrain_padded = pad_sequences(train_sequences, maxlen = maxlen, truncating = trunc_type, padding = pad_type)\n\nval_sequences = tokenizer.texts_to_sequences(val.cleaned_text.values)\nval_padded = pad_sequences(val_sequences, maxlen = maxlen, truncating = trunc_type, padding = pad_type)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:20.789127Z","iopub.execute_input":"2022-03-27T20:51:20.789457Z","iopub.status.idle":"2022-03-27T20:51:28.991183Z","shell.execute_reply.started":"2022-03-27T20:51:20.789414Z","shell.execute_reply":"2022-03-27T20:51:28.990347Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"one_hot_labels = tf.keras.utils.to_categorical(train.type_index.values, num_classes=16)\nval_labels= tf.keras.utils.to_categorical(val.type_index.values, num_classes=16)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:28.992286Z","iopub.execute_input":"2022-03-27T20:51:28.992502Z","iopub.status.idle":"2022-03-27T20:51:28.998522Z","shell.execute_reply.started":"2022-03-27T20:51:28.992477Z","shell.execute_reply":"2022-03-27T20:51:28.997694Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten, Dropout, Conv1D, GlobalMaxPooling1D\n\ndef create_model():\n    op = tf.keras.optimizers.Adam(learning_rate=0.00001)\n\n    model = Sequential()\n    model.add(Embedding(vocab_size, 256, input_length=maxlen-1))\n    model.add(Dropout(0.3))\n    model.add(Bidirectional(LSTM(200, return_sequences=True)))\n    model.add(Dropout(0.3))\n    model.add(Bidirectional(LSTM(20)))\n    model.add(Dropout(0.3))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(16, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer=op, metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:28.999796Z","iopub.execute_input":"2022-03-27T20:51:29.000143Z","iopub.status.idle":"2022-03-27T20:51:29.010833Z","shell.execute_reply.started":"2022-03-27T20:51:29.000112Z","shell.execute_reply":"2022-03-27T20:51:29.010040Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"use_tpu = True\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n    model = create_model()\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:29.011865Z","iopub.execute_input":"2022-03-27T20:51:29.012390Z","iopub.status.idle":"2022-03-27T20:51:37.540399Z","shell.execute_reply.started":"2022-03-27T20:51:29.012358Z","shell.execute_reply":"2022-03-27T20:51:37.539711Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"model.fit(train_padded, one_hot_labels, epochs =20, verbose = 1, \n          validation_data = (val_padded, val_labels),  callbacks = [tf.keras.callbacks.EarlyStopping(patience = 3)])","metadata":{"execution":{"iopub.status.busy":"2022-03-27T20:51:37.541604Z","iopub.execute_input":"2022-03-27T20:51:37.542177Z","iopub.status.idle":"2022-03-27T21:00:27.571600Z","shell.execute_reply.started":"2022-03-27T20:51:37.542129Z","shell.execute_reply":"2022-03-27T21:00:27.570689Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}